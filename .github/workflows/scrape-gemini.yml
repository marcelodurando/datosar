name: Scrape scratch pages (Fixed Commit)

on:
  workflow_dispatch:
    inputs:
      sitemap_url:
        description: "Sitemap URL"
        required: true
        type: string
        default: "https://lottoedge.com/florida-lottery-sitemap.xml"

permissions:
  contents: write

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Install and Run Cloudflare WARP
        uses: fscarmen/warp-on-actions@v1.1
        with:
          stack: dual  # Enable IPv4 and IPv6

      - name: Check new IP (Optional, just to verify)
        run: curl -s https://ifconfig.me

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install Dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable
          
          # Detect Chrome Version
          CHROME_VERSION=$(google-chrome --version | grep -oP '\d+' | head -1)
          echo "Detected Chrome Version: $CHROME_VERSION"
          echo "CHROME_VERSION_MAIN=$CHROME_VERSION" >> $GITHUB_ENV
          
          pip install undetected-chromedriver selenium

      - name: Run Scraper Script
        env:
          SITEMAP_URL: ${{ inputs.sitemap_url }}
          CHROME_VERSION_MAIN: ${{ env.CHROME_VERSION_MAIN }}
        run: |
          mkdir -p datos/scratches
          
          cat <<EOF > scraper.py
          import os
          import time
          import random
          import re
          import undetected_chromedriver as uc
          from selenium.webdriver.common.by import By

          sitemap_url = os.environ["SITEMAP_URL"]
          chrome_version = int(os.environ["CHROME_VERSION_MAIN"])
          output_dir = "datos/scratches"
          
          print(f"üîß Initializing Undetected Chrome Driver for Chrome v{chrome_version}...")
          
          options = uc.ChromeOptions()
          options.add_argument('--headless=new') 
          options.add_argument('--no-sandbox')
          options.add_argument('--disable-dev-shm-usage')
          options.add_argument('--window-size=1920,1080')
          options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36')
          
          driver = uc.Chrome(options=options, version_main=chrome_version)

          urls = []
          try:
              print(f"üöÄ Visiting sitemap via Browser: {sitemap_url}")
              driver.get(sitemap_url)
              time.sleep(10)
              content = driver.page_source
              print(content)
              print("üîç Parsing URLs from source...")
              found_urls = re.findall(r'(https://lottoedge\.com/[\w\-]+/scratch/[\w\-]+/)', content)
              urls = list(set(found_urls))
              print(f"‚úÖ Found {len(urls)} unique scratch URLs.")

          except Exception as e:
              print(f"‚ùå Error fetching sitemap: {e}")
              driver.quit()
              exit(1)

          if not urls:
              print("‚ùå No URLs found.")
              driver.quit()
              exit(0)

          count = 0
          max_count = 5

          for url in urls:
              if count >= max_count:
                  break
              count += 1
              print(f"[{count}/{max_count}] Navigating to: {url}")
              
              try:
                  driver.get(url)
                  time.sleep(random.uniform(8, 15))
                  
                  if "Just a moment" in driver.title:
                       time.sleep(10)

                  html_content = driver.page_source
                  safe_filename = url.replace("https://", "").replace("http://", "").replace("/", "_").strip("_") + ".html"
                  file_path = os.path.join(output_dir, safe_filename)
                  
                  with open(file_path, "w", encoding="utf-8") as f:
                      f.write(html_content)
                  print(f"üíæ Saved: {safe_filename}")
              except Exception as e:
                  print(f"‚ùå Failed to scrape {url}: {e}")

          driver.quit()
          EOF

          python scraper.py
          
          # Cleanup script so it doesn't appear in git status
          rm scraper.py

      - name: Commit downloaded HTML files
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          # Check if directory exists
          if [ -d "datos/scratches" ]; then
             git add datos/scratches
             
             # üîç CHECK IF THERE ARE CHANGES STAGED
             if ! git diff --cached --exit-code; then
               echo "üìù Changes detected. Committing..."
               git commit -m "Add scratch page HTML snapshots [skip ci]"
               git push
             else
               echo "‚úÖ No changes detected (files match existing repo). Skipping commit."
             fi
          else
             echo "‚ö†Ô∏è No data directory found."
          fi

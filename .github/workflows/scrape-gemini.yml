name: Scrape scratch pages (Full Browser Mode)

on:
  workflow_dispatch:
    inputs:
      sitemap_url:
        description: "Sitemap URL"
        required: true
        type: string
        default: "https://lottoedge.com/florida-lottery-sitemap.xml"

permissions:
  contents: write

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          # Install Chrome
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable
          
          # Install Python libraries
          # We need undetected-chromedriver to bypass CF
          pip install undetected-chromedriver selenium

      - name: Run Scraper Script
        env:
          SITEMAP_URL: ${{ inputs.sitemap_url }}
        run: |
          mkdir -p datos/scratches
          
          cat <<EOF > scraper.py
          import os
          import time
          import random
          import re
          import undetected_chromedriver as uc
          from selenium.webdriver.common.by import By

          sitemap_url = os.environ["SITEMAP_URL"]
          output_dir = "datos/scratches"
          
          print("üîß Initializing Undetected Chrome Driver...")
          
          options = uc.ChromeOptions()
          # HEADLESS MODE: 'new' is required to bypass detection
          options.add_argument('--headless=new') 
          options.add_argument('--no-sandbox')
          options.add_argument('--disable-dev-shm-usage')
          options.add_argument('--window-size=1920,1080')
          options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36')
          
          # Initialize driver
          driver = uc.Chrome(options=options)

          urls = []

          try:
              print(f"üöÄ Visiting sitemap via Browser: {sitemap_url}")
              driver.get(sitemap_url)
              
              # WAIT for Cloudflare challenge to pass
              print("‚è≥ Waiting 10 seconds for Cloudflare...")
              time.sleep(10)
              
              # Get page content
              content = driver.page_source
              
              # Use Regex to find URLs because Chrome might wrap XML in HTML tags
              # Looking for https://lottoedge.com/.../scratch/...
              # This pattern looks for the specific structure of scratch off urls
              print("üîç Parsing URLs from source...")
              found_urls = re.findall(r'(https://lottoedge\.com/[\w\-]+/scratch/[\w\-]+/)', content)
              
              # Deduplicate
              urls = list(set(found_urls))
              print(f"‚úÖ Found {len(urls)} unique scratch URLs.")

          except Exception as e:
              print(f"‚ùå Error fetching sitemap: {e}")
              driver.quit()
              exit(1)

          if not urls:
              print("‚ùå No URLs found. Dumping page source for debugging:")
              print(content[:500]) # Print first 500 chars to see if blocked
              driver.quit()
              exit(1)

          # --- Start Scraping Pages ---
          count = 0
          max_count = 5

          for url in urls:
              if count >= max_count:
                  print("üõë Limit of 5 reached.")
                  break
              
              count += 1
              print(f"[{count}/{max_count}] Navigating to: {url}")
              
              try:
                  driver.get(url)
                  
                  # ‚è≥ Human wait time (8-15 seconds)
                  sleep_time = random.uniform(8, 15)
                  time.sleep(sleep_time)
                  
                  # Check if trapped in CF challenge
                  if "Just a moment" in driver.title:
                       print("‚ö†Ô∏è  Caught in challenge loop. Waiting extra 10s...")
                       time.sleep(10)

                  html_content = driver.page_source
                  
                  # Simple validation: Check if content is too short (likely a block page)
                  if len(html_content) < 1000:
                      print("‚ö†Ô∏è  Page content surprisingly small. Might be blocked.")

                  # Save file
                  safe_filename = url.replace("https://", "").replace("http://", "").replace("/", "_").strip("_") + ".html"
                  file_path = os.path.join(output_dir, safe_filename)
                  
                  with open(file_path, "w", encoding="utf-8") as f:
                      f.write(html_content)
                      
                  print(f"üíæ Saved: {safe_filename}")

              except Exception as e:
                  print(f"‚ùå Failed to scrape {url}: {e}")

          print("üèÅ Done. Closing driver.")
          driver.quit()
          EOF

          python scraper.py

      - name: Commit downloaded HTML files
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          if [ -d "datos/scratches" ] && [ "$(ls -A datos/scratches)" ]; then
             git add datos/scratches
             git commit -m "Add scratch page HTML snapshots [skip ci]"
             git push
          else
             echo "No data downloaded, skipping commit."
          fi

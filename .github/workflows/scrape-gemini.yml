name: Scrape scratch pages (Python/Selenium)

on:
  workflow_dispatch:
    inputs:
      sitemap_url:
        description: "Sitemap URL"
        required: true
        type: string

permissions:
  contents: write

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          # Install Chrome (needed for the driver)
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable
          
          # Install Python libraries
          pip install undetected-chromedriver selenium requests lxml

      - name: Run Scraper Script
        env:
          SITEMAP_URL: ${{ inputs.sitemap_url }}
        run: |
          # Create output directory
          mkdir -p datos/scratches
          
          # Create and run the python script inline
          cat <<EOF > scraper.py
          import os
          import time
          import random
          import requests
          import undetected_chromedriver as uc
          from selenium.webdriver.common.by import By
          from xml.etree import ElementTree

          sitemap_url = os.environ["SITEMAP_URL"]
          output_dir = "datos/scratches"
          
          print(f"üöÄ Starting scraper for: {sitemap_url}")

          # 1. Fetch Sitemap using standard requests (Cloudflare usually allows sitemaps)
          # If this fails, we can use the driver for this too.
          try:
              headers = {"User-Agent": "Mozilla/5.0"}
              response = requests.get(sitemap_url, headers=headers)
              if response.status_code != 200:
                  print(f"‚ùå Failed to fetch sitemap: {response.status_code}")
                  exit(1)
              
              # Parse XML
              root = ElementTree.fromstring(response.content)
              # Extract URLs containing '/scratch/'
              # Namespace handling for sitemaps
              urls = []
              for child in root:
                  url = child[0].text
                  if '/scratch/' in url:
                      urls.append(url)
              
              print(f"‚úÖ Found {len(urls)} scratch URLs in sitemap")
          except Exception as e:
              print(f"‚ùå Error parsing sitemap: {e}")
              exit(1)

          if not urls:
              print("No URLs found.")
              exit(0)

          # 2. Setup Undetected Chromedriver
          options = uc.ChromeOptions()
          options.add_argument('--headless=new') # New headless mode is less detectable
          options.add_argument('--no-sandbox')
          options.add_argument('--disable-dev-shm-usage')
          options.add_argument('--window-size=1920,1080')
          
          print("üîß Initializing Chrome Driver...")
          driver = uc.Chrome(options=options, version_main=114) # Force version if needed, or let auto-detect

          count = 0
          max_count = 5

          for url in urls:
              if count >= max_count:
                  print("üõë Limit of 5 reached.")
                  break
              
              count += 1
              print(f"[{count}/{max_count}] Navigating to: {url}")
              
              try:
                  driver.get(url)
                  
                  # ‚è≥ Random sleep to look human and let Cloudflare JS challenge finish
                  sleep_time = random.uniform(8, 15)
                  time.sleep(sleep_time)
                  
                  # Check if we are still on a challenge page
                  title = driver.title.lower()
                  if "just a moment" in title or "attention required" in title:
                      print("‚ö†Ô∏è Still stuck on Cloudflare challenge. Waiting longer...")
                      time.sleep(10)

                  # Get HTML source
                  html_content = driver.page_source
                  
                  # Save file
                  safe_filename = url.replace("https://", "").replace("http://", "").replace("/", "_") + ".html"
                  file_path = os.path.join(output_dir, safe_filename)
                  
                  with open(file_path, "w", encoding="utf-8") as f:
                      f.write(html_content)
                      
                  print(f"üíæ Saved: {safe_filename}")

              except Exception as e:
                  print(f"‚ùå Failed to scrape {url}: {e}")

          driver.quit()
          EOF

          # Run the python script
          python scraper.py

      - name: Commit downloaded HTML files
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          if [ -d "datos/scratches" ] && [ "$(ls -A datos/scratches)" ]; then
             git add datos/scratches
             git commit -m "Add scratch page HTML snapshots [skip ci]"
             git push
          else
             echo "No data downloaded, skipping commit."
          fi

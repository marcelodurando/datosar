name: Scrape scratch pages from sitemap

on:
  workflow_dispatch:
    inputs:
      sitemap_url:
        description: "Sitemap URL"
        required: true
        type: string

permissions:
  contents: write

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Install curl-impersonate
        run: |
          mkdir -p $HOME/ci-tool
          cd $HOME/ci-tool
          
          # Download valid release (fixed URL typo from previous attempts)
          wget -O curl-impersonate.tar.gz https://github.com/lwthiker/curl-impersonate/releases/download/v0.6.1/curl-impersonate-v0.6.1.x86_64-linux-gnu.tar.gz
          
          tar -xvf curl-impersonate.tar.gz
          echo "$HOME/ci-tool" >> $GITHUB_PATH

      - name: Fetch sitemap and extract scratch URLs
        run: |
          mkdir -p tmp datos/scratches

          # Standard Browser Headers for Sitemap fetch
          curl-impersonate-chrome \
            --compressed \
            --location \
            -k \
            -c tmp/cookies.txt \
            -H "Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8" \
            -H "Accept-Language: en-US,en;q=0.9" \
            "${{ inputs.sitemap_url }}" \
            -o tmp/sitemap.xml

          # Extract URLs (using || true to prevent crash)
          grep -oP '(?<=<loc>).*?(?=</loc>)' tmp/sitemap.xml | grep '/scratch/' > tmp/scratch_urls.txt || true

          if [ -s tmp/scratch_urls.txt ]; then
            count=$(wc -l < tmp/scratch_urls.txt)
            echo "Found $count scratch URLs (Processing first 5)"
          else
            echo "No scratch URLs found."
            exit 1
          fi

      - name: Download scratch pages (Max 5)
        run: |
          i=0
          
          # Extract the base domain to use as a realistic Referer (e.g., https://site.com/)
          # This looks more like a human clicking from the homepage than from sitemap.xml
          ROOT_DOMAIN=$(echo "${{ inputs.sitemap_url }}" | awk -F/ '{print $1"//"$3"/"}')

          while read -r url; do
            i=$((i+1))
            
            echo "[$i/5] Fetching $url"
            
            # Create safe filename
            filename=$(echo "$url" | sed 's|https\?://||; s|/|_|g').html

            curl-impersonate-chrome \
              --compressed \
              --location \
              -k \
              -b tmp/cookies.txt \
              -c tmp/cookies.txt \
              -H "Referer: $ROOT_DOMAIN" \
              -H "Upgrade-Insecure-Requests: 1" \
              -H "Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7" \
              -H "Accept-Language: en-US,en;q=0.9" \
              -H "Cache-Control: max-age=0" \
              "$url" \
              -o "datos/scratches/$filename"

            # ðŸ›‘ STOP AFTER 5 DOWNLOADS
            if [ "$i" -ge 5 ]; then
              echo "Reached limit of 5 downloads. Stopping."
              break
            fi

            # â³ Sleep 5 to 15 seconds (Humans rarely wait 0 seconds)
            SLEEP_TIME=$(( 5 + RANDOM % 11 ))
            echo "Sleeping for $SLEEP_TIME seconds..."
            sleep $SLEEP_TIME
            
          done < tmp/scratch_urls.txt

      - name: Commit downloaded HTML files
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          # Check if there are files in the specific scratches folder
          if [ -d "datos/scratches" ] && [ "$(ls -A datos/scratches)" ]; then
             git add datos/scratches
             git commit -m "Add scratch page HTML snapshots [skip ci]"
             git push
          else
             echo "No data downloaded, skipping commit."
          fi
